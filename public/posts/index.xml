<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on yadnyesh&#39;s blog</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on yadnyesh&#39;s blog</description>
    <generator>Hugo -- 0.147.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 28 Dec 2025 12:23:11 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Beyond PPO - The New Wave of Policy Optimization for LLM Post-Training</title>
      <link>http://localhost:1313/posts/policy_optimization/</link>
      <pubDate>Sun, 28 Dec 2025 12:23:11 +0530</pubDate>
      <guid>http://localhost:1313/posts/policy_optimization/</guid>
      <description>&lt;p&gt;PPO used to be the default workhorse for RLHF because it’s reasonably stable and easy to reason about, but at LLM post-training scale its tradeoffs start to bite: the critic/value model is expensive to train and maintain, long text rollouts amplify variance and make advantage estimation brittle, clipping becomes a blunt instrument that can under-update (wasting samples) or over-update (destabilizing), and the whole loop turns into a systems-heavy exercise once “environment interaction” means generating thousands of tokens across distributed inference. As post-training shifted from short preference tuning toward &lt;strong&gt;reasoning-heavy objectives&lt;/strong&gt; (RLVR, long-CoT, verifier-driven rewards, pass@k-style targets) and larger, more heterogeneous data mixtures, these weaknesses became harder to paper over with hyperparameter folklore because the optimization problem is noisier, the feedback signals are sparser, and the failure modes (reward hacking, length bias, mode collapse, over-regularization) are more punishing. That’s why the field has been moving beyond “just PPO” toward &lt;strong&gt;more robust, more LLM-native policy optimization&lt;/strong&gt;: methods that reduce dependence on a critic, stabilize updates under long-horizon generation, better control distribution shift between samples and policy, and align the training objective with how we actually evaluate modern models, ultimately making post-training not just &lt;em&gt;possible&lt;/em&gt;, but &lt;em&gt;reliable&lt;/em&gt; under the messy realities of large-scale reasoning optimization.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dissecting FlashInfer - A Systems Perspective on High-Performance LLM Inference</title>
      <link>http://localhost:1313/posts/flash_infer/</link>
      <pubDate>Thu, 09 Oct 2025 00:00:00 +0530</pubDate>
      <guid>http://localhost:1313/posts/flash_infer/</guid>
      <description>&lt;p&gt;The next frontier of large language model optimization isn’t architectural - it’s infrastructural. We’ve squeezed what we can from model design; now, inference efficiency is dictated by how we map computation to hardware. The challenge is executing them with minimal memory movement, maximal kernel fusion and predictable latency across heterogeneous batches. Every inefficiency (redundant projection, scattered memory access, unaligned kernels) compounds at scale. The gap between theoretical FLOPs and delivered throughput is now a systems problem.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Geometric Analysis of Transformer Representations via Optimal Transport</title>
      <link>http://localhost:1313/posts/opt_trans_rep/</link>
      <pubDate>Mon, 08 Sep 2025 12:34:29 +0530</pubDate>
      <guid>http://localhost:1313/posts/opt_trans_rep/</guid>
      <description>&lt;p&gt;Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of &lt;em&gt;what actually happens between layers as a model learns to reason&lt;/em&gt;. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Go With The Flow</title>
      <link>http://localhost:1313/posts/go_with_the_flow/</link>
      <pubDate>Sun, 27 Apr 2025 12:34:29 +0530</pubDate>
      <guid>http://localhost:1313/posts/go_with_the_flow/</guid>
      <description>&lt;p&gt;Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their &lt;a href=&#34;https://bfl.ai/models/flux-pro&#34;&gt;FLUX&lt;/a&gt; models and &lt;a href=&#34;https://stability.ai/news/introducing-stable-diffusion-3-5&#34;&gt;SD3.5&lt;/a&gt; model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
