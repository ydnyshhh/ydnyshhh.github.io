<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>yadnyesh&#39;s blog</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on yadnyesh&#39;s blog</description>
    <generator>Hugo -- 0.147.0</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 09 Oct 2025 00:00:00 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Dissecting FlashInfer - A Systems Perspective on High-Performance LLM Inference</title>
      <link>http://localhost:1313/posts/flash_infer/</link>
      <pubDate>Thu, 09 Oct 2025 00:00:00 +0530</pubDate>
      <guid>http://localhost:1313/posts/flash_infer/</guid>
      <description>&lt;p&gt;The next frontier of large language model optimization isn’t architectural - it’s infrastructural. We’ve squeezed what we can from model design; now, inference efficiency is dictated by how we map computation to hardware. The challenge is executing them with minimal memory movement, maximal kernel fusion and predictable latency across heterogeneous batches. Every inefficiency (redundant projection, scattered memory access, unaligned kernels) compounds at scale. The gap between theoretical FLOPs and delivered throughput is now a systems problem.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Geometric Analysis of Transformer Representations via Optimal Transport</title>
      <link>http://localhost:1313/posts/opt_trans_rep/</link>
      <pubDate>Mon, 08 Sep 2025 12:34:29 +0530</pubDate>
      <guid>http://localhost:1313/posts/opt_trans_rep/</guid>
      <description>&lt;p&gt;Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of &lt;em&gt;what actually happens between layers as a model learns to reason&lt;/em&gt;. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Go With The Flow</title>
      <link>http://localhost:1313/posts/go_with_the_flow/</link>
      <pubDate>Sun, 27 Apr 2025 12:34:29 +0530</pubDate>
      <guid>http://localhost:1313/posts/go_with_the_flow/</guid>
      <description>&lt;p&gt;Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their &lt;a href=&#34;https://bfl.ai/models/flux-pro&#34;&gt;FLUX&lt;/a&gt; models and &lt;a href=&#34;https://stability.ai/news/introducing-stable-diffusion-3-5&#34;&gt;SD3.5&lt;/a&gt; model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reading List</title>
      <link>http://localhost:1313/readinglist/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/readinglist/</guid>
      <description>&lt;p&gt;Here&amp;rsquo;s my collection of favorite readings:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Children of Time - Adrian Tchaikovsky&lt;/li&gt;
&lt;li&gt;Anathem - Neal Stephenson&lt;/li&gt;
&lt;li&gt;Seveneves - Neal Stephenson&lt;/li&gt;
&lt;li&gt;Permutation City - Greg Egan&lt;/li&gt;
&lt;li&gt;Project Hail Mary - Andy Weir&lt;/li&gt;
&lt;li&gt;Hyperion - Dan Simmons&lt;/li&gt;
&lt;li&gt;Rendezvous with Rama - Arthur C Clarke&lt;/li&gt;
&lt;li&gt;Stories of Your Life and Others - Ted Chiang&lt;/li&gt;
&lt;li&gt;There is no Antimemetics Division - qntm&lt;/li&gt;
&lt;li&gt;Dune - Frank Herbert&lt;/li&gt;
&lt;li&gt;Recursion - Blake Crouch&lt;/li&gt;
&lt;li&gt;Neuromancer - William Gibson&lt;/li&gt;
&lt;li&gt;Red Rising - Pierce Brown&lt;/li&gt;
&lt;li&gt;Vagabond - Takehiko Inoue&lt;/li&gt;
&lt;li&gt;The Beginning Of Infinity - David Deutsch&lt;/li&gt;
&lt;li&gt;The Fabric of Reality - David Deutsch&lt;/li&gt;
&lt;li&gt;The Man from the Future - Ananyo Bhattacharya&lt;/li&gt;
&lt;li&gt;Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter&lt;/li&gt;
&lt;li&gt;The Upanishads - Eknath Easwaran&lt;/li&gt;
&lt;li&gt;Essays On The Gita - Sri Aurobindo&lt;/li&gt;
&lt;li&gt;Shriman Yogi - Ranjit Desai&lt;/li&gt;
&lt;li&gt;Siddhartha - Hermann Hesse&lt;/li&gt;
&lt;li&gt;Napoleon - Andrew Roberts&lt;/li&gt;
&lt;li&gt;Why Greatness Cannot be Planned - Springer&lt;/li&gt;
&lt;li&gt;Boom : Bubbles and the End of Stagnation - Stripe Press&lt;/li&gt;
&lt;li&gt;The Dream Machine - Stripe Press&lt;/li&gt;
&lt;li&gt;A Mind at Play (Claude Shannon) - Jimmy Soni&lt;/li&gt;
&lt;li&gt;SPQR - A History of Ancient Rome - Mary Beard&lt;/li&gt;
&lt;li&gt;Focus - The ASML Way&lt;/li&gt;
&lt;li&gt;The Nvidia Way - Tae Kim&lt;/li&gt;
&lt;li&gt;Chip War - Chris Miller&lt;/li&gt;
&lt;li&gt;The Worlds I See - Fei Fei Li&lt;/li&gt;
&lt;li&gt;Surely You’re Joking Mr. Feynman - Richard Feynman&lt;/li&gt;
&lt;li&gt;The Republic - Plato&lt;/li&gt;
&lt;li&gt;The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
  </channel>
</rss>
