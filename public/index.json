[{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_# P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_# P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$ where $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$ where $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\text{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\left{ \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\right} $$ where, $$ H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$ Here, C is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and ε\u0026gt;0 is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_# P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_# P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$ where $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\text{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\left{ \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\right} $$ where, $$ H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$ Here, C is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and ε\u0026gt;0 is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_# P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T:; T_# P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$ where $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\text{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\left{ \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\right} $$ where, $$ H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$ Here, C is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and ε\u0026gt;0 is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_# P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T:; T_# P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$ where $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\text{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\left{ \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\right} $$ where, $$ H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$ Here, C is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and ε\u0026gt;0 is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_# P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_# P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$ where $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\text{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\left{ \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\right} $$ where, $$ H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$ Here, C is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and ε\u0026gt;0 is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_# P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_# P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$ where $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\text{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\left{ \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\right} $$ where, $$ H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$ Here, C is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and ε\u0026gt;0 is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_# P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_# P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$ where $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\text{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\left{ \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\right} $$ where, $$ H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$ Here, C is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and ε\u0026gt;0 is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_# P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_# P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$ where $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\text{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\left{ \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\right} $$ where, $$ H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$ Here, C is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and ε\u0026gt;0 is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_# P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$ where $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\text{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\left{ \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\right} $$ where, $$ H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$ Here, C is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and ε\u0026gt;0 is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_# P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$ where $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\text{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\left{ \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\right} $$ where, $$ H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$ Here, C is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and ε\u0026gt;0 is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$ where $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\text{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\left{ \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\right} $$ where, $$ H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$ Here, C is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and ε\u0026gt;0 is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\text{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\left{ \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\right} $$ where, $$ H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$ Here, C is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and ε\u0026gt;0 is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\text{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\left{ \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\right}, \\quad \\text{where } H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\text{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\left{ \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\right}, \\quad \\text{where } H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\mathrm{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\Bigg{ \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Bigg}, \\quad \\text{where } H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\mathrm{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\Bigg{ \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Bigg}, \\quad \\text{where } H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\mathrm{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\Big( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Big), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\mathrm{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\Big( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Big), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\mathrm{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\Bigl( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Bigr), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ \\mathrm{OT}\\varepsilon(P, Q) = \\min{\\gamma \\in \\Pi(P, Q)} \\Bigl( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Bigr), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log \\gamma_{ij} $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ OT_\\varepsilon(P, Q) = \\min_{\\gamma \\in \\Pi(P, Q)} \\Big( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Big), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log(\\gamma_{ij}) $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ OT_\\varepsilon(P, Q) = \\min_{\\gamma \\in \\Pi(P, Q)} \\Big( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Big), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log(\\gamma_{ij}) $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ OT_\\varepsilon(P, Q) = \\min_{\\gamma \\in \\Pi(P, Q)} \\Big( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Big), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log(\\gamma_{ij}) $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ OT_\\varepsilon(P, Q) = \\min_{\\gamma \\in \\Pi(P, Q)} \\Big( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Big), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log(\\gamma_{ij}) $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ OT_\\varepsilon(P, Q) = \\min_{\\gamma \\in \\Pi(P, Q)} \\Big( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Big), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log(\\gamma_{ij}) $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ OT_\\varepsilon(P, Q) = \\min_{\\gamma \\in \\Pi(P, Q)} \\Big( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Big), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log(\\gamma_{ij}) $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ OT_\\varepsilon(P, Q) = \\min_{\\gamma \\in \\Pi(P, Q)} \\Big( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Big), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log(\\gamma_{ij}) $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ OT_\\varepsilon(P, Q) = \\min_{\\gamma \\in \\Pi(P, Q)} \\Big( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Big), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log(\\gamma_{ij}) $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ OT_\\varepsilon(P, Q) = \\min_{\\gamma \\in \\Pi(P, Q)} \\Big( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Big), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log(\\gamma_{ij}) $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ OT_\\varepsilon(P, Q) = \\min_{\\gamma \\in \\Pi(P, Q)} \\Big( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Big), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log(\\gamma_{ij}) $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ OT_\\varepsilon(P, Q) = \\min_{\\gamma \\in \\Pi(P, Q)} \\Big( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Big), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log(\\gamma_{ij}) $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nAcknowledgements I would like to thank Apart Research, PIBBSS, and Timaeus for hosting the Apart AI Safety x Physics Challenge 2025, which initiated this project. I am also grateful to Sunishka Sharma (Adobe), Janhavi Khindkar (IIIT Hyderabad), and Vishnu Vardhan Lanka (Independent Researcher), who were members of my team.\nApart Labs provided funding and support for this research, without which this work would not have been possible. I would also like to thank Jesse Hoogland, Ari Brill, and Esben Kran for providing insightful feedback on our initial draft.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ OT_\\varepsilon(P, Q) = \\min_{\\gamma \\in \\Pi(P, Q)} \\Big( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Big), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log(\\gamma_{ij}) $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nAcknowledgements I would like to thank Apart Research, PIBBSS, and Timaeus for hosting the Apart AI Safety x Physics Challenge 2025, which initiated this project. I am also grateful to Sunishka Sharma (Adobe), Janhavi Khindkar (IIIT Hyderabad), and Vishnu Vardhan Lanka (Independent Researcher), who were members of my team.\nApart Labs provided funding and support for this research, without which this work would not have been possible. I would also like to thank Jesse Hoogland, Ari Brill, and Esben Kran for providing insightful feedback on our initial draft.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"},{"content":"Transformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of what actually happens between layers as a model learns to reason. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\nBridging Information Theory and Representation Geometry One of the most influential perspectives for understanding deep neural networks (DNNs) comes from information theory, pioneered by Professor Naftali Tishby and collaborators. Instead of treating neural networks as inscrutable black boxes, this framework casts them as information processing systems. The central idea is simple but powerful: learning in a DNN is not just about memorizing patterns; it is about shaping how information flows, compresses, and refines across layers.\nAt its core, a neural network can be viewed as a Markov chain: $$ X \\rightarrow h_1 \\rightarrow h_2 \\rightarrow \\cdots \\rightarrow h_m \\rightarrow \\hat{Y} $$ Each layer depends only on the previous one. This structure has important consequences. By the Data Processing Inequality (DPI), mutual information, the measure of how much one variable tells us about another, can only decrease as we move forward through the chain. In practice, this means that as raw input passes through successive layers, the network must compress away irrelevant details while retaining what matters for predicting Y.\nThis intuition is formalized in the Information Bottleneck (IB) principle. The goal of a hidden representation is twofold:\nPreserve as much relevant information about the target as possible: $I(\\hat{X};Y)$ Discard irrelevant information from the input: $I(X; \\hat{X})$ The IB framework describes this as an optimization problem: find the “sweet spot” where representations are maximally compressed yet maximally predictive. In other words, the network learns to forget just enough of the input to generalize well, while keeping what is essential for the task.\nTo visualize this process, Tishby introduced the information plane, where each hidden layer is plotted based on two values: mutual information with the input: $I(X; T_i)$ and with the output: $I(T_i; Y)$. During training, layers trace characteristic trajectories across this plane. Early on, they rapidly climb upwards, capturing more information about the target (the fitting phase). Later, they drift leftwards, compressing redundant details from the input (the compression phase). This two-stage dynamic suggests that generalization in deep learning is inseparable from information compression.\nBeyond interpretation, the IB framework also provides new theoretical insights. Traditional complexity bounds (e.g., VC dimension) fail to explain why massive over-parameterized models can generalize so well. But by framing generalization error in terms of mutual information, the IB view shows that compression directly improves generalization, each bit of discarded input information effectively multiplies the sample efficiency.\nLimitations\nWhile the IB theory is an elegant explanation for learning, its primary challenge has been empirical validation. The theory relies on measuring mutual information $I(X; T_i)$ and $I(T_i; Y)$, which is notoriously difficult to calculate accurately for high-dimensional, continuous data like neural network activations. This has led to debates about whether the observed \u0026ldquo;compression phase\u0026rdquo; is a genuine phenomenon or an artifact of the measurement technique, especially in modern networks that use ReLU activations.\nHow Optimal Transport Provides a Solution? The starting point for this research was a question that has long fascinated us: what does it really mean for a neural network to “learn” internally? Information theory, particularly the Information Bottleneck (IB) framework pioneered by Naftali Tishby, gave us our first conceptual lens. It suggested that learning is a process of compressing input representations while preserving task-relevant information, effectively building an efficient “information highway” from inputs to outputs. The idea that networks undergo phases of fitting and compression, visible on the information plane, framed our thinking: learning is not just parameter tuning, but a reorganization of information flow.\nAt the same time, we were intrigued by another mathematical tradition: Optimal Transport (OT). Originally formulated by Gaspard Monge in the 18th century and later relaxed by Leonid Kantorovich, OT asks: what is the most efficient way to move mass from one distribution to another? This perspective felt deeply aligned with our information-theoretic intuitions. If layer activations in a neural network can be seen as empirical distributions, then the transformation from one layer to the next is nothing more than a transport problem. The “cost” of moving one distribution into another could give us a direct, geometric measure of how much work the network performs between layers.\nA third piece of inspiration came from recent advances in generative modeling, specifically flow matching and rectified flows. In these frameworks, Conditional Optimal Transport (CondOT) has been used to construct continuous transformations between distributions in a way that respects underlying geometry. This showed us that OT is not just a mathematical curiosity, but a practical and scalable tool for studying representation dynamics in high-dimensional spaces.\nThese three perspectives: the compression principle of information theory, the geometric ruler of optimal transport, and the scalable implementations from flow-based models came together to shape our approach.\nOptimal Transport as a Geometric Lens on Transformer Representations To analyze how representations evolve across layers in a neural network, we need a measure of \u0026ldquo;distance\u0026rdquo; between two sets of high-dimensional points. Traditional metrics such as Euclidean or Cosine distance can track changes at the level of individual vectors, but they fall short in capturing the collective, structural shift of the entire representation distribution. This motivates the use of Optimal Transport (OT).\nMonge’s Formulation: The Earth Mover’s Problem The OT problem dates back to Gaspard Monge (1781), who posed it as a question of logistics: How can we move a pile of earth (distribution $P$) to fill a hole of another shape (distribution $Q$) at minimal cost?\nFormally, let $P$ and $Q$ be probability distributions over a space $\\mathcal{X}$. Monge’s formulation asks us to find a transport map $T:\\mathcal{X}→\\mathcal{X}$ such that pushing forward $P$ by $T$ gives $Q$: $T_\\sharp P = Q$\nand the goal is to minimize the total cost of transport:\n$$ \\inf_{T: T_\\sharp P = Q} \\int_{\\mathcal{X}} c(x, T(x)) , dP(x) $$\nwhere $c(x,y)$ is the cost of moving one unit of mass from $x$ to $y$.\nWhile elegant, Monge’s formulation is highly restrictive: it requires a deterministic map $T$, which may not exist if mass must be split.\nKantorovich’s Relaxation: Probabilistic Transport Leonid Kantorovich (1942) introduced a relaxation that makes OT much more flexible. Instead of searching for a transport map, he considered transport plans: joint probability distributions $\\gamma(x, y)$ on $\\mathcal{X} \\times \\mathcal{X}$ whose marginals are $P$ and $Q$. Intuitively, $\\gamma(x, y)$ describes how much mass at location $x$ is assigned to location $y$.\nThe problem becomes: $$ \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} c(x, y) , d\\gamma(x, y) $$\nwhere $\\Pi(P, Q)$ is the set of all valid couplings of $P$ and $Q$.\nWith $c(x, y) = |x-y|^2$, this gives us the 2-Wasserstein distance: $$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{\\mathcal{X} \\times \\mathcal{X}} |x-y|^2 , d\\gamma(x, y) $$ This is a true metric on the space of probability distributions, sensitive to geometry, and often interpreted as the minimal “work” needed to transform one distribution into another.\nMaking OT Practical: Sinkhorn Regularization Computing the exact Wasserstein distance is computationally expensive in high dimensions. To make it tractable, we adopt entropic regularization, which adds a penalty term based on the entropy of the transport plan: $$ OT_\\varepsilon(P, Q) = \\min_{\\gamma \\in \\Pi(P, Q)} \\Big( \\sum_{i,j} \\gamma_{ij} C_{ij} - \\varepsilon H(\\gamma) \\Big), \\quad H(\\gamma) = - \\sum_{i,j} \\gamma_{ij} \\log(\\gamma_{ij}) $$\nHere, $C$ is the cost matrix where $C_{jk} = | h_i^{(j)} - h_{i+1}^{(k)} |^2$, and $\\varepsilon \u0026gt; 0$ is the regularization parameter.\nThis regularized version can be solved efficiently with the Sinkhorn–Knopp algorithm, yielding what is commonly called the Sinkhorn distance. In practice, this makes OT scalable to the high-dimensional distributions we encounter in neural networks. We utilize the Python Optimal Transport (POT) library for this computation. This method is not only faster but also statistically more robust for high-dimensional data, making it the ideal choice for our analysis.\nThis figure illustrates how entropic regularization makes optimal transport (OT) computation fast and stable - from the paper Sinkhorn Distances: Lightspeed Computation of Optimal Transport. The classical OT solution $P^\\star$ (green) lies at a sharp vertex of the transport polytope, making it expensive to compute. By adding an entropy term, Cuturi smoothed the problem, shifting the solution into the interior (red, $P^\\lambda$), where it can be efficiently found using Sinkhorn iterations. As $\\lambda \\to 0$, the solution approaches the maximum-entropy independent coupling $rc^T$ (blue), and as $\\lambda \\to \\infty$, it recovers the true OT solution. In practice, the Sinkhorn distance provides a controllable approximation that trades exactness for lightspeed computation.\nCalculating Representation Distance with Optimal Transport To apply optimal transport (OT) to transformer activations, we treat the problem as comparing two distributions: the representation at layer $L_i$ and the representation at the next layer $L_{i+1}$. The process unfolds in several steps:\n1. Form empirical distributions: For a given batch of inputs, we collect the activation vectors at layer $L_i$, denoted ${x_1, \\dots, x_n}$, and the activation vectors at layer $L_{i+1}$, denoted ${y_1, \\dots, y_m}$. These two sets of vectors serve as empirical samples from distributions $P_n$ and $Q_m$.\n2. Compute the cost matrix:\nWe construct a cost matrix $C \\in \\mathbb{R}^{n \\times m}$, where each entry corresponds to the squared Euclidean distance between a source activation and a target activation: $C_{jk} = | x_j - y_k |^2_2$\nThis captures the pairwise “transport cost” between activations across adjacent layers.\n3. Calculate the Sinkhorn distance:\nUsing the cost matrix $C$ and uniform probability weights for both empirical distributions, we compute the entropically regularized OT cost via the Sinkhorn algorithm. The resulting scalar value is what we call the OT distance between layer $L_i$ and $L_{i+1}$. Intuitively, this number reflects the geometric “work” required to reshape the distribution of activations from one layer into the next.\n4. Repeat across the network:\nWe apply this procedure to every adjacent pair of layers in the model, yielding a full profile of OT distances across depth.\n5. Compare across training:\nFinally, we run the same analysis at different stages of training — both on randomly initialized (untrained) models and fully trained models. This lets us isolate the structural changes in representation flow that emerge specifically as a result of learning.\nResearch Findings and Analysis The Learned Strategy of the Trained Model When we examined the OT distances across layers in a trained model, a clear pattern emerged: the pathway of representations is not uniform but instead follows a distinct U-shaped profile. This suggests that the model organizes its internal computations into three functional phases:\n1. Encoding Phase (Initial Layers):\nThe largest geometric transformations happen at the very beginning. The OT distance spikes between the first few layers, reflecting the effort needed to project raw embeddings into the model’s internal representational space. Soon after, this cost drops sharply, signaling that the network quickly consolidates the most important input features.\n2. Refinement Phase (Middle Layers):\nOnce the initial heavy lifting is done, the model enters a long, stable region where OT distances remain low. Here, the network is no longer radically reshaping the geometry; instead, it is subtly refining and recombining the features it has already extracted. This stage resembles iterative reasoning, where abstract patterns are composed with relatively little geometric “work.”\n3. Decoding Phase (Final Layers):\nThe final transition is dramatic: OT distance spikes once again at the output end of the model. This sharp increase reflects the transformation of the abstract internal state into the final task-specific output space. In other words, the last layer acts as a specialized projection head that carries the most geometrically significant burden of all.\nInterestingly, this three-phase pattern aligns with how the model manages representation entropy. Just as OT distances follow a U-shape, entropy plunges after the initial encoding, bottoms out in the middle layers, and rises again as the model projects into the output space. This suggests that the model has effectively learned to create an information bottleneck: compressing inputs into low-entropy, task-relevant states in the middle layers before expanding them for the final output.\nThe Random Walk of the Untrained Model The story looks very different in an untrained model. Without learning, the OT profile lacks any structured phases. Distances are uniformly high, noisy, and erratic across all layers. Every layer appears to perform a large, random transformation on its input, with no clear signs of consolidation or refinement.\nEntropy dynamics tell a similar story. Instead of showing a bottleneck, entropy in the untrained model collapses abruptly at the start, then steadily increases without structure, essentially reflecting noise accumulation. This chaotic trajectory is best described as a random walk through representation space — in sharp contrast to the purposeful, three-phase strategy of the trained model.\nThe Emergence of the Information Bottleneck: A comparison of layer-wise representation entropy reveals that the trained model (blue) learns to compress information into a low-entropy state for refinement. In contrast, the untrained model (red) remains in a state of high disorder, demonstrating that the bottleneck is a learned, not inherent, property of the architecture.\nFuture Work and Experiments Exploring Universality and Scalability Different Architectures: We suggest testing whether this U-shaped profile appears in other architectures. An experiment could apply this OT analysis to: Vision Transformers (ViTs), BERT-style encoder-only models, or T5-style encoder-decoder models to see if this \u0026ldquo;signature of learning\u0026rdquo; is a universal trait of transformers.\nScaling Laws: How does the geometry change with model size? One could plot the OT curves for a family of models to see if the \u0026ldquo;refinement valley\u0026rdquo; gets deeper or longer as model capability increases. This would connect the geometric view to the well-known scaling laws.\nTask-Specific Dynamics: Analyze OT and entropy for models fine-tuned on different tasks (translation, summarization, reasoning) to see if the structure adapts.\nConnecting Geometry to Model Behavior Emergent Abilities: The GSM8K dataset tests mathematical reasoning, an emergent ability. A fascinating experiment would be to track the OT profile during training. At the specific training step where the model\u0026rsquo;s accuracy on a task suddenly improves, is there a corresponding sudden change in the geometric profile, like the formation of the information bottleneck?\nFine-Tuning vs. Pre-training: Analyze and compare the OT profile of a pre-trained foundation model with its profile after being fine-tuned on a specific task. The hypothesis would be that pre-training establishes the deep U-shape, while fine-tuning primarily modifies the final \u0026ldquo;decoding\u0026rdquo; layers. This would provide geometric evidence for how transfer learning works.\nAcknowledgements I would like to thank Apart Research, PIBBSS, and Timaeus for hosting the Apart AI Safety x Physics Challenge 2025, which initiated this project. I am also grateful to Sunishka Sharma (Adobe), Janhavi Khindkar (IIIT Hyderabad), and Vishnu Vardhan Lanka (Independent Researcher), who were members of my team.\nApart Labs provided funding and support for this research, without which this work would not have been possible. I would also like to thank Jesse Hoogland, Ari Brill, and Esben Kran for providing insightful feedback on our initial draft.\nReferences A Geometric Analysis of Transformer Representations via Optimal Transport Deep Learning and the Information Bottleneck Principle Sinkhorn Distances: Lightspeed Computation of Optimal Transport A Visual Dive into Conditional Flow Matching ","permalink":"http://localhost:1313/posts/opt_trans_rep/","summary":"\u003cp\u003eTransformer models have become the backbone of modern AI, yet their remarkable performance still comes with a critical limitation: we lack a clear understanding of how information is processed inside them. Traditional evaluation focuses on outputs, but this leaves open the deeper question of \u003cem\u003ewhat actually happens between layers as a model learns to reason\u003c/em\u003e. In our work, we approach this problem through a geometric lens, using Optimal Transport to measure how entire distributions of representations shift across layers. This perspective allows us to contrast trained and untrained models, revealing that training does not simply tune parameters, but organizes computation into a structured three-phase strategy: encoding, refinement, and decoding, underpinned by an information bottleneck. By making this internal structure visible, we aim to move closer to principled interpretability, where understanding a model means understanding the pathways of information it discovers through learning.\u003c/p\u003e","title":"A Geometric Analysis of Transformer Representations via Optimal Transport"},{"content":"Flow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their FLUX models and SD3.5 model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\nThis wave of renewed interest is pushing researchers to rethink how we model generative processes—not just as noisy reversals but as guided, purposeful flows through data space. Unlike diffusion models that slowly denoise data step by step, flow-based models aim to learn the exact path from noise to structure in one go, which can make generation faster and more controllable. Plus, with growing support from recent papers and open-source projects, it’s becoming easier for developers and researchers to experiment with these models and push them into new creative and scientific applications.\nSo What’s a Flow? Imagine you\u0026rsquo;re watching a river flow downstream. At every point, the water has a direction and speed—it’s not just sitting still. Now picture a tiny leaf floating on that river. As time passes, the river carries the leaf smoothly along a path. That motion—the direction the leaf moves at each point in time—is what we’d call a flow.\nIn the world of generative AI, we can think of data points like that little leaf, and the flow is a kind of invisible force field that guides how we transform one kind of data (say, pure noise) into another (like a realistic image). Instead of just jumping from noise to image, the model learns how to gradually reshape the randomness into structure, just like the river reshapes the leaf’s position as it moves.\nSo, just like in a river or a gust of wind, every point in space has a little arrow—a velocity vector—that tells tiny particles where to move. And just like how water currents or wind patterns can change over time and space, our model’s “flow field” can also depend on both position and time.\nFlow matching taps into this idea by trying to learn those natural movement patterns. It’s like asking at every moment, “If this data point were a leaf on a river, which direction should it drift right now?” The model doesn’t just guess the final destination—it learns the smooth path each point should follow through the data space.\nA Quick Linear Algebra Refresher Before we dive deeper into the mathematical intuition behind flows and how flow-based models actually work, let’s hit pause and take a quick detour to understand what is a Jacobian and the Change of Variables theorem.\nGiven a function of mapping a $n$-dimensional input vector $\\mathbf{x}$ to a $m$-dimensional output vector, $f : \\mathbb{R}^n \\mapsto \\mathbb{R}^m$, the matrix of all first-order partial derivatives of this function is called the Jacobian matrix, $\\mathbf{J}$, where one entry on the $i$-th row and $j$-th column is\n$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$ The Jacobian matrix looks like: $$ J = \\begin{bmatrix} \\displaystyle \\frac{\\partial f_1}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\displaystyle \\frac{\\partial f_m}{\\partial x_1} \u0026amp; \\cdots \u0026amp; \\displaystyle \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $$\nThe Jacobian tells us how a function transforms space locally—it shows how small changes in input affect each output. It acts like a map of sensitivities and stretching, and its determinant measures how much the function expands or contracts space at a point.\nThe determinant is a single number that summarizes certain properties of a square matrix. You can only compute it for square matrices—that is, matrices with the same number of rows and columns.\nAt a high level, the determinant tells us how much a matrix stretches or squashes space.\nYou can think of it as measuring the \u0026ldquo;volume change\u0026rdquo; caused by the matrix when it transforms data. For an $n \\times n$ matrix $M$, the determinant is calculated as:\n$$ \\det M = \\begin{vmatrix} a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\ a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn} \\end{vmatrix} = \\sum_{j_1 j_2 \\cdots j_n} (-1)^{\\tau(j_1 j_2 \\cdots j_n)} a_{1j_1} a_{2j_2} \\cdots a_{nj_n} $$\nThis sum runs over all possible permutations of column indices $j_1, j_2, \\dots, j_n$.\nThe function $\\tau(\\cdot)$ gives the sign of each permutation (+1 or -1), depending on how \u0026ldquo;twisted\u0026rdquo; it is.\nWhy is the determinant useful?\nIf $\\det(M) = 0$, the matrix flattens space in some direction—it’s not reversible. This means the matrix is not invertible (it’s singular). If $\\det(M) \\ne 0$, then the matrix preserves some volume, and is invertible. A neat property of determinants:\nIf you multiply two square matrices, their determinants also multiply:\n$$ \\det(AB) = \\det(A) \\cdot \\det(B) $$\nChange of Variables Theorem (for Density Estimation)\nAt it\u0026rsquo;s core Change of Variable theorem tells us how a probability density changes when you transform the variable it\u0026rsquo;s defined on.\nLet’s go over the change of variable theorem in the context of probability density estimation, starting with the simple case of a single variable.\nOne-Dimensional Case\nSuppose we have a random variable $z$ with known density $\\pi(z)$, and we define a new variable $x = f(z)$ using a 1-to-1 invertible function $f$. This means $z = f^{-1}(x)$.\nNow we want to find the unknown density of $x$, denoted $p(x)$.\nUsing the fact that probabilities must sum to 1:\n$$ \\int p(x) \\ dx = \\int \\pi(z) \\ dz = 1 $$\nTo convert densities between variables, we apply the change of variables formula:\n$$ p(x) = \\pi(z) \\left| \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\frac{d f^{-1}}{dx} \\right| = \\pi(f^{-1}(x)) \\left| (f^{-1})\u0026rsquo;(x) \\right| $$\nHere, $\\left| \\frac{df^{-1}}{dx} \\right|$ captures how the space is stretched or squashed during transformation.\nThink of the integral $\\int \\pi(z) , dz$ as adding up many thin rectangles. Each rectangle has Width $\\Delta z$ and Height $\\pi(z)$. When we substitute $z = f^{-1}(x)$, we’re essentially changing the coordinates. The width becomes:$$ \\Delta z = \\left( f^{-1}(x) \\right)\u0026rsquo; \\Delta x $$So, the scaling factor $\\left| (f^{-1}(x))\u0026rsquo; \\right|$ tells us how the density stretches when changing variables.\nMultivariable Case\nFor higher dimensions, the concept extends using the Jacobian determinant.\nLet: $$ z \\sim \\pi(z), \\quad x = f(z), \\quad z = f^{-1}(x) $$\nThen the density transforms as:\n$$ p(x) = \\pi(z) \\left| \\det \\frac{dz}{dx} \\right| = \\pi(f^{-1}(x)) \\left| \\det \\frac{d f^{-1}}{dx} \\right| $$\nHere, $\\det \\frac{\\partial f}{\\partial x}$ is the Jacobian determinant of the function $f$.\nNormalizing Flows Normalizing Flows is a method for turning a simple probability distribution (like a Gaussian) into a complex one by applying a sequence of invertible and differentiable transformations.\nNormalizing Flows learn an invertible mapping $f: \\mathcal{X} \\rightarrow \\mathcal{Z}$ ; where $\\mathcal{X}$ is the data distribution and $\\mathcal{Z}$ is a chosen latent distribution.\nLet:\n$$ x = f_\\theta(z) = f_k \\circ \\cdots \\circ f_2 \\circ f_1(z) $$\nwhere each $f_i$ is invertible (bijective).\nWe define:\n$$ f: \\mathcal{Z} \\rightarrow \\mathcal{X}, \\quad \\text{where } f \\text{ is invertible} $$\nLet $p_\\theta(x)$ be the probability density over $x$, with $z \\in \\mathcal{Z}$.\nChange of Variable Formula:\n$$ p_\\theta(x) = p_\\theta(f^{-1}(x)) \\left| \\det \\left( \\frac{\\partial f^{-1}(x)}{\\partial x} \\right) \\right| $$\nIf we replace $f^{-1}(x)$ with $z$, the formula becomes:\n$$ p_\\theta(x) = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial z}{\\partial x} \\right) \\right| $$ Finally:\n$$ p_\\theta(x) = p_\\theta(z) \\prod_{i=1}^{k} \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| = p_\\theta(z) \\left| \\det \\left( \\frac{\\partial f^{-1}}{\\partial x} \\right) \\right| $$\nThe sequence of transformations applied to random variables, expressed as:\n$$ z_i = f_i(z_{i-1}) $$\nis called the flow. When this sequence forms a chain of distributions $\\pi_i$, the entire process is referred to as a normalizing flow. For each transformation function $f_i$ in the flow to be usable in practice, it must meet two key criteria:\nIt must be easily invertible. Its Jacobian determinant must be efficient to compute. Why Are Normalizing Flows Called \u0026ldquo;Normalizing\u0026rdquo; Flows? Normalizing Flows refer to the process of transforming probability distributions while preserving normalization—i.e., ensuring they remain valid probability distributions. The change of variables formula ensures that the probability density is adjusted (or “normalized”) correctly during this transformation. This adjustment is what allows the transformed distribution to stay normalized—hence the name \u0026ldquo;Normalizing Flows\u0026rdquo;.\nExact Log-Likelihoods with Normalizing Flows One of the key advantages of Normalizing Flows is that they allow us to compute exact log-likelihoods, which is rare for most generative models.\nVariational Autoencoders (VAEs) Provide only a lower bound on the log-likelihood (ELBO). Use an approximate posterior $q_\\phi(z \\mid x)$ Generative Adversarial Networks (GANs) Do not support log-likelihood evaluation. Do not provide latent variable inference. Normalizing Flows With normalizing flows, we can compute:\n$$ \\log p_\\theta(x) = \\log p_\\theta(z) + \\sum_{i=1}^K \\log \\left| \\det \\left( \\frac{\\partial f_i^{-1}}{\\partial z_i} \\right) \\right| $$\nAllows for exact log-likelihood evaluation. Also enables exact posterior inference via the invertible transformation $z = f^{-1}(x)$ Flow Matching - A Deep Dive Given a training dataset sampled from a target distribution $q$ over $\\mathbb{R}^d$, the goal is to learn a generative model that can produce new samples from $q$. To achieve this, Flow Matching (FM) constructs a probability path $(p_t)_{0 \u0026lt; t \u0026lt; 1}$ that transitions from a known source distribution $p_0 = p$ to the target distribution $p_1 = q$, where each $p_t$ is a distribution over $\\mathbb{R}^d$.\nFlow Matching works by training a velocity field—a neural network that estimates the instantaneous velocity of samples along this path. This field is later used to transport samples from the source $p_0$ to the target $p_1$ by solving a differential equation.\nAfter training, generating a new sample from the target distribution $X_1 \\sim q$ involves:\nSampling an initial point $X_0 \\sim p$, and Solving the Ordinary Differential Equation (ODE) guided by the learned velocity field. Why to solve the ODE? Given a starting point $X_0$ ODE tells you how to move it to $X_1$.\nA flow $\\psi_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ (represented by the square grid) is defined through a velocity field $u_t : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, visualized here using blue arrows. This velocity field dictates how every point moves instantaneously across space. The three images illustrate how the grid is progressively deformed at different time steps $t$.\nThe objective of generative flow modeling is to learn a flow $\\psi_t$ such that:\n$$ X_1 = \\psi_1(X_0) \\sim q, $$\nwhere $X_0$ is sampled from a simple source distribution (e.g., Gaussian), and $q$ is the target data distribution.\nNote: Flow and Diffusion Models Both Flow Models and Diffusion Models generate samples by evolving a random variable over time, starting from a simple initial distribution.\nFlow Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} \\quad \\text{(e.g., Gaussian)} $$ Evolve using an ODE:\n$$ dX_t = u_t^\\theta(X_t)\\ dt $$\n$u_t^\\theta$ is a neural network that defines a time-dependent vector field. The evolution is deterministic (no randomness). Diffusion Model Initialize:\n$$ X_0 \\sim p_{\\text{init}} $$ Evolve using an SDE: $$ dX_t = u_t^\\theta(X_t)\\ dt + \\sigma_t\\ dW_t $$\n$u_t^\\theta$ is again a neural network vector field. $\\sigma_t$ is the diffusion coefficient. $dW_t$ is a Wiener process (i.e., standard Brownian motion). The evolution is stochastic, involving both learned dynamics and noise. Constructing the Training Target for Flow Models Typically, we train the model by minimizing a mean squared error:\n$$ L(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 $$\nHere, $u_t^{\\text{target}}(x)$ is the training target we want the model\u0026rsquo;s prediction $u_t^\\theta(x)$ to match.\nIn standard regression or classification, the training target is usually a label. But in this case, we don’t have labels. So instead, we need to derive the training target ourselves.\nConditional Probability Path Dirac Distribution:\nLet: $z \\in \\mathbb{R}^d, \\quad \\delta_z \\Rightarrow X = z$ Here, $\\delta_z$ is the Dirac distribution, which assigns all probability mass to the single point $z$. This implies $X$ takes the exact value $z$.\nWe define a conditional probability path $p_t(\\cdot \\mid z)$ such that $p_t(\\cdot \\mid z)$ is a distribution over $\\mathbb{R}^d$ The path starts at: $p_0(\\cdot \\mid z) = p_{\\text{init}}$ and ends at $p_1(\\cdot \\mid z) = \\delta_z$ This means we begin from an initial distribution and evolve it into a Dirac delta centered at $z$.\nExample : Gaussian Probability Path - An example of such a conditional path is a Gaussian with time-varying mean and variance: $\\mathcal{N}(\\alpha_t z,\\ \\beta_t^2 I_d)$. This describes a path where samples are centered around $\\alpha_t z$ and the uncertainty (spread) is controlled by $\\beta_t^2$.\nNote : Conditional means per single data point and marginal means across distribution of data points\nConditional Vector Field We define a target conditional vector field:\n$$ u_t^{\\text{target}}(x \\mid z), \\quad \\text{for } 0 \\leq t \\leq 1,\\ x, z \\in \\mathbb{R}^d $$\nsuch that: $$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt}X_t = u_t^{\\text{target}}(X_t \\mid z) \\quad \\Rightarrow \\quad X_t \\sim p_t(\\cdot \\mid z) ;\\quad 0 \\leq t \\leq 1 $$\n$X$ is the variable of interest (what we want to evolve or sample). $z$ is the conditioning variable. The vector field $u_t^{\\text{target}}(x \\mid z)$ is time-dependent (indexed by $t$), and it guides the evolution of $X$ conditioned on $z$. If we start with samples from a known distribution and move them using this conditional vector field, then the distribution of the evolved particles at time $t$ will follow the conditional target distribution $X_t \\sim p_t(x \\mid z)$.\nStart by sampling $X_0$ from an initial distribution $p_{\\text{init}}$. Then, evolve the samples over time using the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ by solving the following ODE:\n$$ \\frac{dX_t}{dt} = u_t^{\\text{target}}(X_t \\mid z) $$\nAs the system evolves, at any time $t$, the samples $X_t$ should follow the desired conditional probability distribution: $$ X_t \\sim p_t(x \\mid z) $$ Marginal Probability Path We define the marginal path $p_t$ as the marginalization over conditional distributions.\nGiven:\n$z \\sim p_{\\text{data}}$ and $X \\sim p_t(\\cdot \\mid z)$ -\u0026gt; then $X \\sim p_t$ To obtain the marginal distribution $p_t(X)$, we integrate over $z$:\n$$ p_t(X) = \\int p_t(X \\mid z) \\ p_{\\text{data}}(z) \\ dz $$\nWe define the probability path with:\n$p_0 = p_{\\text{init}}$ (start from noise) $p_1 = p_{\\text{data}}$ (end at real data) This defines a smooth transformation from noise → data over time, as illustrated in the figure with intermediate distributions $p_t$ between $t = 0$ and $t = 1$\nMarginal Vector Field We can define the marginal target vector field $u_t^{\\text{target}}(x)$ by averaging the conditional vector field $u_t^{\\text{target}}(x \\mid z)$ weighted by the joint distribution:\n$$ u_t^{\\text{target}}(x) = \\int u_t^{\\text{target}}(x \\mid z) \\cdot \\frac{p_t(x \\mid z) \\cdot p_{\\text{data}}(z)}{p_t(x)} \\ dz $$\nThis equation uses the conditional vector field and transforms it into a marginal one over x. This satisfies: If we sample from the initial distribution and evolve using this marginal vector field:\n$$ X_0 \\sim p_{\\text{init}}, \\quad \\frac{d}{dt} X_t = u_t^{\\text{target}}(X_t) $$\nThen the samples $X_t$ follow the marginal distribution:\n$$ X_t \\sim p_t \\quad \\text{for } t \\in [0, 1] $$\nAt $t = 1$, we reach the target:\n$$ X_1 \\sim p_{\\text{data}} $$\nThis method is often referred to as the \u0026ldquo;Marginalization Trick\u0026rdquo;, where we compute a marginal vector field $u_t^{\\text{target}}(x)$ given the conditional one $u_t^{\\text{target}}(x \\mid z)$.\nConditional \u0026amp; Marginal Score Function In generative modeling, a score function refers to the gradient of the log-probability with respect to the input. It points in the direction where the data density increases—useful for guiding the sampling process.\n1. Conditional Score $$ \\nabla_x \\log p_t(x \\mid z) $$ Represents the gradient of the log-density of $x$ given a conditioning variable $z$. It tells us how likely $x$ is, assuming we already know $z$.\n2. Marginal Score $$ \\nabla_x \\log p_t(x) $$ Represents the gradient of the marginal log-density of $x$, where $z$ is integrated out. It is used in unconditional models that learn the overall data distribution without conditioning. Central to score-based diffusion models and other likelihood-free approaches.\nFlow Matching Now that we\u0026rsquo;ve covered the core ideas behind flow models—like what a flow is, how normalizing flows work, and the roles of conditional and marginal probability paths and vector fields—let\u0026rsquo;s dive into what Flow Matching actually is.\nThe goal of Flow Matching is to learn a neural vector field $u_t^\\theta$ that closely matches the target vector field $u_t^{\\text{target}}$:\n$$ u_t^\\theta \\approx u_t^{\\text{target}} $$\nFlow Matching Loss The objective in flow matching is to minimize the difference between the learned vector field $u_t^\\theta(x)$ and the target vector field $u_t^{\\text{target}}(x)$. The ideal loss function is:\n$$ \\mathcal{L}_{\\text{fm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x) \\right|^2 \\right] $$\nTo evaluate this loss (in theory), we would:\nSample $t \\sim \\text{Uniform}[0, 1]$ Sample $z \\sim p_{\\text{data}}$ (a real data point) Sample $x \\sim p_t(\\cdot \\mid z)$ from the conditional path The challenge is that we cannot directly compute this loss, because the true target vector field $u_t^{\\text{target}}(x)$ is unknown or intractable. So what\u0026rsquo;s the solution? Conditional Flow Matching Loss!\nConditional Flow Matching Loss We define the Conditional Flow Matching (CFM) loss as:\n$$ \\mathcal{L}_{\\text{cfm}}(\\theta) = \\mathbb{E} \\left[ \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 \\right] $$\nWhere:\n$t \\sim \\text{Uniform}[0, 1]$ $z \\sim p_{\\text{data}}$ (draw a data sample) $x \\sim p_t(\\cdot \\mid z)$ (draw from conditional probability path) Theorem:\nThere exists a constant $c \u0026gt; 0$ (independent of $\\theta$) such that:\n$$ L_{\\text{fm}}(\\theta) = L_{\\text{cfm}}(\\theta) + c $$\nThis means minimizing the conditional loss is equivalent (up to a constant) to minimizing the original flow matching loss.\nKey Consequences The minimizer $\\theta^*$ of $\\mathcal{L}_{\\text{cfm}}$ satisfies: $$ u_t^{\\theta^*} = u_t^{\\text{target}} $$\nThe gradients are the same: $$ \\nabla_\\theta L_{\\text{cfm}}(\\theta) = \\nabla_\\theta L_{\\text{fm}}(\\theta) $$\nSo **Stochastic Gradient Descent (SGD)** behaves the same for both losses and the training trajectories will not differ. All we need to do is minimize a simple mean squared error loss, which is much easier to optimize than adversarial objectives like those in GANs (which require min-max optimization).\nFlow Matching Training Procedure (General) Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network $u_t^\\theta$ (the learnable vector field) Training Loop\nFor each mini-batch of data:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample $x \\sim p_t(\\cdot \\mid z)$ — from the conditional path at time $t$. Compute the flow matching loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - u_t^{\\text{target}}(x \\mid z) \\right|^2 $$ Update model parameters $\\theta$ using gradient descent on $\\mathcal{L}(\\theta)$. Flow Matching Training for CondOT (Optimal Transport) Path Given:\nA dataset of samples $z \\sim p_{\\text{data}}$ A neural network vector field $u_t^\\theta$ Training Procedure\nFor each mini-batch:\nSample a data example $z$ from the dataset. Sample a random time $t \\sim \\text{Uniform}[0, 1]$. Sample noise $\\epsilon \\sim \\mathcal{N}(0, I_d)$. Compute: $$ x = t z + (1 - t) \\epsilon $$ Compute the loss: $$ \\mathcal{L}(\\theta) = \\left| u_t^\\theta(x) - (z - \\epsilon) \\right|^2 $$ Update model parameters $\\theta$ via gradient descent on $\\mathcal{L}(\\theta)$. Rectified Flows Since distributions are at the heart of statistics and machine learning, many core problems—like generative modeling and domain transfer—can be understood through the lens of finding a transport map that moves one distribution to another. Rectified Flow offers a simple way to do this by learning an ordinary differential equation (ODE), also known as a flow model, with the central idea of encouraging movement along straight paths as much as possible. This approach is closely connected to neural ODEs and stochastic differential equation (SDE) models, particularly the widely used diffusion generative models and their ODE variants.\nTraditionally, there are infinitely many ODEs or SDEs that could map between two distributions, and most methods implicitly choose a trajectory without a clear principle. In contrast, rectified flow explicitly prefers ODEs whose solution paths are straight lines—what they call straight flows. This leads to a simple, principled framework that ties naturally to optimal transport theory. A major advantage is that straight flows eliminate discretization error when solving ODEs numerically, meaning rectified flows allow very fast inference—sometimes achievable in just a single Euler step. As a result, they combine the speed of one-step generative models like GANs and VAEs with the robustness and training stability of multi-step ODE/SDE-based models.\nLearning Transport Maps The Transport Mapping Problem:\nGiven empirical observations of two distributions $\\pi_0, \\pi_1$ on $\\mathbb{R}^d$,\nthe goal is to find a transport map:\n$$ T: \\mathbb{R}^d \\to \\mathbb{R}^d $$\nsuch that, in the infinite data limit, $Z_1 := T(Z_0) \\sim \\pi_1$ when $Z_0 \\sim \\pi_0$.\nIn other words, $(Z_0, Z_1)$ forms a coupling (also called a transport plan) between $\\pi_0$ and $\\pi_1$.\nGenerative Modeling:\n$\\pi_1$ is an unknown data distribution (e.g., images), $\\pi_0$ is a simple known distribution (e.g., standard Gaussian). The goal is to learn a nonlinear transform that maps samples from $\\pi_0$ to samples from $\\pi_1$. Transfer Modeling:\nBoth $\\pi_0$ and $\\pi_1$ are unknown empirical distributions. The goal is to transfer data points from $\\pi_0$ to $\\pi_1$, or vice versa. Applications include domain adaptation, transfer learning, image editing, and sim2real in robotics. Why is Finding a Transport Map Challenging? Given two distributions, there are infinitely many possible transport maps $T$.\nThe goal is to find one that transfers $\\pi_0$ to $\\pi_1$ and has desirable properties, such as high computational efficiency and practical simplicity\nTo achieve this, we often formulate the problem mathematically to impose additional desirable properties.\nOptimal Transport (OT) One canonical formulation is Optimal Transport (OT), where we seek a transport plan that minimizes a cost.\nSpecifically, Monge\u0026rsquo;s Optimal Transport problem is:\n$$ \\min_T \\mathbb{E} \\left[ c(T(Z_0) - Z_0) \\right] \\quad \\text{subject to} \\quad \\text{Law}(Z_0) = \\pi_0, \\quad \\text{Law}(T(Z_0)) = \\pi_1 $$ where:\n$c: \\mathbb{R}^d \\to \\mathbb{R}$ is a cost function (e.g., $c(x) = \\frac{1}{2} |x|^2$), $\\mathbb{E}\\left[ c(T(Z_0) - Z_0) \\right]$ measures the expected transport cost. Think of $Z_0$ and $Z_1$ as two piles of sand, and $c(Z_1 - Z_0)$ as the cost of moving sand from $Z_0$ to $Z_1$.\nHowever, solving the optimal transport (OT) problem remains highly challenging, especially when dealing with high-dimensional data and large-scale datasets. Developing efficient algorithms for these settings is still an open problem.\nMoreover, in generative and transfer modeling, the transport cost itself is often not the primary focus — the learning performance is not directly tied to the magnitude of $Z_1 - Z_0$. While optimal transport maps do induce smoothness properties that are beneficial for learning, minimizing transport cost isn\u0026rsquo;t the ultimate goal.\nMethod: Rectified Flow Rectified flow learns the transport map $T$ implicitly by constructing an\nordinary differential equation (ODE) driven by a drift force:\n$$ dZ_t = v(Z_t, t) dt, \\quad t \\in [0, 1], \\quad \\text{starting from} \\quad Z_0 \\sim \\pi_0 $$\nThe goal is to ensure that when we follow the ODE starting from $Z_0 \\sim \\pi_0$,\nwe end up with $Z_1 \\sim \\pi_1$.\nThe main challenge is how to construct the drift field $v$ based only on observations from $\\pi_0$and $\\pi_1$, typically using deep neural networks or other nonlinear approximators.\nAt first glance, this looks hard. One natural idea is to find $v$ by minimizing a discrepancy measure $D(\\rho_1^v, \\pi_1)$, where $\\rho_1^v$ is the distribution of $Z_1$ after following the ODE driven by $v$ and $D(\\cdot, \\cdot)$ is some divergence, like KL divergence.\nHowever, this requires:\nRepeated simulation of the ODE, Inferring intermediate states, which is computationally expensive, And the big problem: we don\u0026rsquo;t know what intermediate trajectories the ODE should travel through! We can avoid these difficulties by exploiting the over-parameterized nature of the problem. Since we only care about matching the start ($\\pi_0$) and end ($\\pi_1$) distributions,\nthe intermediate states $\\pi_t$ of $Z_t$ can be any smooth interpolation between $\\pi_0$ and $\\pi_1$. Thus, we can (and should) inject strong assumptions about the intermediate paths.\nThe simplest and most effective assumption? Straight trajectories.\nWhy Straight Paths? Theoretically: They align well with ideas from optimal transport. Computationally: ODEs following straight paths have zero discretization error,\nmeaning they can be solved exactly or with very few numerical steps. How Rectified Flow Works Specifically, rectified flow works by finding an ODE to match the marginal distributions of the linear interpolation of points between $\\pi_0$ and $\\pi_1$.\nAssume we observe $X_0 \\sim \\pi_0$ and $X_1 \\sim \\pi_1$. Let $X_t$ for all $t \\in [0,1]$ be the linear (geodesic) interpolation between $X_0$ and $X_1$:\n$$ X_t = t X_1 + (1 - t) X_0 $$\nObserve that $X_t$ follows a simple ODE that already transfers $\\pi_0$ to $\\pi_1$:\n$$ dX_t = (X_1 - X_0)dt $$\nThis means that $X_t$ moves along the line direction $(X_1 - X_0)$ at a constant speed. However, this ODE is non-causal:\nThe update $X_t$ depends on the final state $X_1$, which is unknown at time $t \u0026lt; 1$. When multiple trajectories cross at a point $X_t$, the direction of motion becomes ambiguous and non-unique. Thus, the \u0026ldquo;causal ODE\u0026rdquo; needed for simulation cannot just be the naive ODE in the equation above. Solution: Causalizing the Interpolation We want to \u0026ldquo;causalize\u0026rdquo; the interpolation process $X_t$ by projecting it into the space of causally simulatable ODEs:\n$$ dZ_t = v(Z_t, t) dt $$\nA natural way to do this is to project the velocity field onto a causal one by minimizing an L2 loss:\n$$ \\min_v \\int_0^1 \\mathbb{E} \\left[ | (X_1 - X_0) - v(X_t, t) |^2 \\right] dt $$\nThis finds a drift $v$ that approximates the ideal direction $(X_1 - X_0)$ as closely as possible at each point $(X_t, t)$.\nTheoretical Solution: Conditional Expectation\nThe optimal drift $v(z,t)$ can be written as:\n$$ v(z, t) = \\mathbb{E}[X_1 - X_0 \\mid X_t = z] $$\nThis means $v(z,t)$ is the expected direction of the lines passing through the point $z$ at time $t$. We call the ODE with this $v(z,t)$ the rectified flow induced from $(X_0, X_1)$.\nIn practice:\nWe solve the minimization using standard optimizers like SGD. We parameterize $v$ with a neural network or other function approximator. The conditional expectation $\\mathbb{E}[\\cdot]$ is estimated empirically from samples $(X_0, X_1)$. The trajectories $Z_t$ traced by rectified flow follow the same density path as the original interpolation $X_t$,\nbut they rewire themselves at intersections to maintain causality and avoid non-uniqueness.\nKey Properties of Rectified Flow The ODE trajectories $Z_t$ and the interpolation $X_t$ have the same marginal distributions for all $t \\in [0, 1]$: $$ \\text{Law}(Z_t) = \\text{Law}(X_t), \\quad \\forall t \\in [0, 1]. $$\nThus, $(Z_0, Z_1)$ forms a valid coupling of the distributions $\\pi_0$ and $\\pi_1$.\nThe coupling $(Z_0, Z_1)$ also guarantees no larger transport cost compared to $(X_0, X_1)$ for any convex cost function $c : \\mathbb{R}^d \\to \\mathbb{R}$: $$ \\mathbb{E} \\left[ c(Z_1 - Z_0) \\right] \\leq \\mathbb{E} \\left[ c(X_1 - X_0) \\right], \\quad \\forall \\text{ convex } c. $$\nThe data pair $(X_0, X_1)$ can be an arbitrary (possibly independent) coupling of $\\pi_0$ and $\\pi_1$ Typically, $(X_0, X_1) \\sim \\pi_0 \\times \\pi_1$ is sampled independently from $\\pi_0$ and $\\pi_1$. In contrast, the rectified coupling $(Z_0, Z_1)$ introduces a deterministic dependency because it is induced from an ODE flow. Thus, rectified flow converts an arbitrary coupling into a deterministic coupling, without increasing convex transport costs.\nReflow: Fast Generation with Straight Flows Denote the rectified flow $Z = { Z_t : t \\in [0, 1] }$ induced from $(X_0, X_1)$ by:\n$$ Z = \\text{Rectflow}((X_0, X_1)). $$\nApplying the $\\text{Rectflow}(\\cdot)$ operator recursively yields a sequence of rectified flows:\n$$ Z^{k+1} = \\text{Rectflow}((Z_0^k, Z_1^k)), $$\nwhere $(Z_0^0, Z_1^0) = (X_0, X_1)$. Here, $Z^k$ is the $k$-th rectified flow, induced from $(X_0, X_1)$.\nIn practice:\nWe sample $(Z_0^k, Z_1^k)$ from the current $k$-th rectified flow, Then retrain a new rectified flow from these samples, Each step makes the flow paths straighter. Why This Matters?\nBesides lowering transport cost, this \u0026ldquo;reflow\u0026rdquo; process has an important side effect:\nIt straightens the paths of rectified flows. As $k$ increases, the paths of $Z^k$ become increasingly straight. Key Properties The straightness of a flow $Z$ can be measured by:\n$$ s(Z) = \\int_0^1 \\mathbb{E} \\left[ | Z_1 - Z_0 - v(Z_t, t) |^2 \\right] dt, $$ where:\n$S(Z) = 0$ corresponds to perfectly straight paths. After $k$ iterations, we have: $$ \\min_{k \\leq K} S(Z^k) = O(1/K). $$ Computational Advantage: Flows with nearly straight paths are computationally efficient:\nThey have minimal discretization error. If the ODE $dZ_t = v(Z_t, t) dt$ has straight paths, then: $$ Z_t = Z_0 + t v(Z_0, 0), $$\nMeaning: the ODE can be solved exactly with just a single Euler step!\nThis addresses the slow inference bottleneck of traditional ODE/SDE models. Thus, reflow enables training one-step generative models (like GANs or VAEs) using ODE flows.\nIn this blog, we took a deep dive into the world of flows — exploring what a flow is, understanding normalizing flows, learning about flow matching, and diving into conditional and marginal probability paths and vector fields. We also unpacked rectified flows in detail. Altogether, this has helped us build a strong theoretical foundation and intuition around flows. In the next blog, we\u0026rsquo;ll shift gears and explore different flow-based models and their architectures.\nReferences Flow Matching Guide and Code- Y. Lipman Flow Matching for Generative Modeling - Y. Lipman Normalizing Flows: An Introduction and Review of Current Methods - Ivan Kobyzev Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow - Xingchao Liu Scaling Rectified Flow Transformers for High-Resolution Image Synthesis - Patrick Esser - Stability AI Flow-based Deep Generative Models - Lilian Weng ","permalink":"http://localhost:1313/posts/go_with_the_flow/","summary":"\u003cp\u003eFlow-based generative models are starting to turn heads as a cool alternative to traditional diffusion methods for things like image and audio generation. What makes them stand out is how they learn smooth, efficient paths to transform one distribution into another—basically a neat and mathematically solid way to generate data. They’ve been getting a lot more buzz lately, especially after Black Forest Labs dropped their \u003ca href=\"https://bfl.ai/models/flux-pro\"\u003eFLUX\u003c/a\u003e models and \u003ca href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\"\u003eSD3.5\u003c/a\u003e model by Stability AI. That success has brought fresh attention to the earlier ideas behind Rectified Flows, which first popped up at ICLR 2023.\u003c/p\u003e","title":"Go With The Flow"},{"content":"Here\u0026rsquo;s my collection of favorite readings:\nChildren of Time - Adrian Tchaikovsky Anathem - Neal Stephenson Seveneves - Neal Stephenson Permutation City - Greg Egan Project Hail Mary - Andy Weir Hyperion - Dan Simmons Rendezvous with Rama - Arthur C Clarke Stories of Your Life and Others - Ted Chiang There is no Antimemetics Division - qntm Dune - Frank Herbert Recursion - Blake Crouch Neuromancer - William Gibson Red Rising - Pierce Brown Vagabond - Takehiko Inoue The Beginning Of Infinity - David Deutsch The Fabric of Reality - David Deutsch The Man from the Future - Ananyo Bhattacharya Gödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter The Upanishads - Eknath Easwaran Essays On The Gita - Sri Aurobindo Shriman Yogi - Ranjit Desai Siddhartha - Hermann Hesse Napoleon - Andrew Roberts Why Greatness Cannot be Planned - Springer Boom : Bubbles and the End of Stagnation - Stripe Press The Dream Machine - Stripe Press A Mind at Play (Claude Shannon) - Jimmy Soni SPQR - A History of Ancient Rome - Mary Beard Focus - The ASML Way The Nvidia Way - Tae Kim Chip War - Chris Miller The Worlds I See - Fei Fei Li Surely You’re Joking Mr. Feynman - Richard Feynman The Republic - Plato The Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew ","permalink":"http://localhost:1313/readinglist/","summary":"\u003cp\u003eHere\u0026rsquo;s my collection of favorite readings:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eChildren of Time - Adrian Tchaikovsky\u003c/li\u003e\n\u003cli\u003eAnathem - Neal Stephenson\u003c/li\u003e\n\u003cli\u003eSeveneves - Neal Stephenson\u003c/li\u003e\n\u003cli\u003ePermutation City - Greg Egan\u003c/li\u003e\n\u003cli\u003eProject Hail Mary - Andy Weir\u003c/li\u003e\n\u003cli\u003eHyperion - Dan Simmons\u003c/li\u003e\n\u003cli\u003eRendezvous with Rama - Arthur C Clarke\u003c/li\u003e\n\u003cli\u003eStories of Your Life and Others - Ted Chiang\u003c/li\u003e\n\u003cli\u003eThere is no Antimemetics Division - qntm\u003c/li\u003e\n\u003cli\u003eDune - Frank Herbert\u003c/li\u003e\n\u003cli\u003eRecursion - Blake Crouch\u003c/li\u003e\n\u003cli\u003eNeuromancer - William Gibson\u003c/li\u003e\n\u003cli\u003eRed Rising - Pierce Brown\u003c/li\u003e\n\u003cli\u003eVagabond - Takehiko Inoue\u003c/li\u003e\n\u003cli\u003eThe Beginning Of Infinity - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Fabric of Reality - David Deutsch\u003c/li\u003e\n\u003cli\u003eThe Man from the Future - Ananyo Bhattacharya\u003c/li\u003e\n\u003cli\u003eGödel, Escher, Bach: an Eternal Golden Braid - Douglas Hofstadter\u003c/li\u003e\n\u003cli\u003eThe Upanishads - Eknath Easwaran\u003c/li\u003e\n\u003cli\u003eEssays On The Gita - Sri Aurobindo\u003c/li\u003e\n\u003cli\u003eShriman Yogi - Ranjit Desai\u003c/li\u003e\n\u003cli\u003eSiddhartha - Hermann Hesse\u003c/li\u003e\n\u003cli\u003eNapoleon - Andrew Roberts\u003c/li\u003e\n\u003cli\u003eWhy Greatness Cannot be Planned - Springer\u003c/li\u003e\n\u003cli\u003eBoom : Bubbles and the End of Stagnation - Stripe Press\u003c/li\u003e\n\u003cli\u003eThe Dream Machine - Stripe Press\u003c/li\u003e\n\u003cli\u003eA Mind at Play (Claude Shannon) - Jimmy Soni\u003c/li\u003e\n\u003cli\u003eSPQR - A History of Ancient Rome - Mary Beard\u003c/li\u003e\n\u003cli\u003eFocus - The ASML Way\u003c/li\u003e\n\u003cli\u003eThe Nvidia Way - Tae Kim\u003c/li\u003e\n\u003cli\u003eChip War - Chris Miller\u003c/li\u003e\n\u003cli\u003eThe Worlds I See - Fei Fei Li\u003c/li\u003e\n\u003cli\u003eSurely You’re Joking Mr. Feynman - Richard Feynman\u003c/li\u003e\n\u003cli\u003eThe Republic - Plato\u003c/li\u003e\n\u003cli\u003eThe Singapore Story: Memoirs of Lee Kuan Yew - Lee Kuan Yew\u003c/li\u003e\n\u003c/ol\u003e","title":"Reading List"}]